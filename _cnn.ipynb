{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel\n",
    "from tf_utils import (\n",
    "    time_distributed_dense_layer, temporal_convolution_layer,\n",
    "    sequence_mean, sequence_smape, shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'data',\n",
    "            'is_nan',\n",
    "            'page_id',\n",
    "            'project',\n",
    "            'access',\n",
    "            'agent',\n",
    "            'test_data',\n",
    "            'test_is_nan'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i))) for i in data_cols]\n",
    "\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.95)\n",
    "\n",
    "        print ('train size', len(self.train_df))\n",
    "        print ('val size', len(self.val_df))\n",
    "        print ('test size', len(self.test_df))\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_epochs=num_epochs,\n",
    "            allow_smaller_final_batch=is_test\n",
    "        )\n",
    "        data_col = 'test_data' if is_test else 'data'\n",
    "        is_nan_col = 'test_is_nan' if is_test else 'is_nan'\n",
    "        for batch in batch_gen:\n",
    "            num_decode_steps = 64\n",
    "            full_seq_len = batch[data_col].shape[1]\n",
    "            max_encode_length = full_seq_len - num_decode_steps if not is_test else full_seq_len\n",
    "\n",
    "            x_encode = np.zeros([len(batch), max_encode_length])\n",
    "            y_decode = np.zeros([len(batch), num_decode_steps])\n",
    "            is_nan_encode = np.zeros([len(batch), max_encode_length])\n",
    "            is_nan_decode = np.zeros([len(batch), num_decode_steps])\n",
    "            encode_len = np.zeros([len(batch)])\n",
    "            decode_len = np.zeros([len(batch)])\n",
    "\n",
    "            for i, (seq, nan_seq) in enumerate(zip(batch[data_col], batch[is_nan_col])):\n",
    "                rand_len = np.random.randint(max_encode_length - 365 + 1, max_encode_length + 1)\n",
    "                x_encode_len = max_encode_length if is_test else rand_len\n",
    "                x_encode[i, :x_encode_len] = seq[:x_encode_len]\n",
    "                is_nan_encode[i, :x_encode_len] = nan_seq[:x_encode_len]\n",
    "                encode_len[i] = x_encode_len\n",
    "                decode_len[i] = num_decode_steps\n",
    "                if not is_test:\n",
    "                    y_decode[i, :] = seq[x_encode_len: x_encode_len + num_decode_steps]\n",
    "                    is_nan_decode[i, :] = nan_seq[x_encode_len: x_encode_len + num_decode_steps]\n",
    "\n",
    "            batch['x_encode'] = x_encode\n",
    "            batch['encode_len'] = encode_len\n",
    "            batch['y_decode'] = y_decode\n",
    "            batch['decode_len'] = decode_len\n",
    "            batch['is_nan_encode'] = is_nan_encode\n",
    "            batch['is_nan_decode'] = is_nan_decode\n",
    "\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cnn(TFBaseModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels=32,\n",
    "        skip_channels=32,\n",
    "        dilations=[2**i for i in range(8)]*3,\n",
    "        filter_widths=[2 for i in range(8)]*3,\n",
    "        num_decode_steps=64,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.num_decode_steps = num_decode_steps\n",
    "        super(cnn, self).__init__(**kwargs)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return tf.log(x + 1) - tf.expand_dims(self.log_x_encode_mean, 1)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return tf.exp(x + tf.expand_dims(self.log_x_encode_mean, 1)) - 1\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.x_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.encode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.y_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "        self.decode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.is_nan_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.is_nan_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "\n",
    "        self.page_id = tf.placeholder(tf.int32, [None])\n",
    "        self.project = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        self.log_x_encode_mean = sequence_mean(tf.log(self.x_encode + 1), self.encode_len)\n",
    "        self.log_x_encode = self.transform(self.x_encode)\n",
    "        self.x = tf.expand_dims(self.log_x_encode, 2)\n",
    "\n",
    "        self.encode_features = tf.concat([\n",
    "            tf.expand_dims(self.is_nan_encode, 2),\n",
    "            tf.expand_dims(tf.cast(tf.equal(self.x_encode, 0.0), tf.float32), 2),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y_decode)[0], 1))\n",
    "        self.decode_features = tf.concat([\n",
    "            tf.one_hot(decode_idx, self.num_decode_steps),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, self.num_decode_steps, 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    def encode(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-encode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-encode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-encode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n",
    "\n",
    "        return y_hat, conv_inputs[:-1]\n",
    "\n",
    "    def initialize_decode_params(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-decode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-decode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-decode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n",
    "        return y_hat\n",
    "\n",
    "    def decode(self, x, conv_inputs, features):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # initialize state tensor arrays\n",
    "        state_queues = []\n",
    "        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n",
    "            batch_idx = tf.range(batch_size)\n",
    "            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n",
    "            batch_idx = tf.reshape(batch_idx, [-1])\n",
    "\n",
    "            queue_begin_time = self.encode_len - dilation - 1\n",
    "            temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n",
    "            temporal_idx = tf.reshape(temporal_idx, [-1])\n",
    "\n",
    "            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n",
    "            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n",
    "\n",
    "            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n",
    "            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n",
    "            state_queues.append(layer_ta)\n",
    "\n",
    "        # initialize feature tensor array\n",
    "        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n",
    "        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n",
    "\n",
    "        # initialize output tensor array\n",
    "        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n",
    "\n",
    "        # initialize other loop vars\n",
    "        elements_finished = 0 >= self.decode_len\n",
    "        time = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        # get initial x input\n",
    "        current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n",
    "        initial_input = tf.gather_nd(x, current_idx)\n",
    "\n",
    "        def loop_fn(time, current_input, queues):\n",
    "            current_features = features_ta.read(time)\n",
    "            current_input = tf.concat([current_input, current_features], axis=1)\n",
    "\n",
    "            with tf.variable_scope('x-proj-decode', reuse=True):\n",
    "                w_x_proj = tf.get_variable('weights')\n",
    "                b_x_proj = tf.get_variable('biases')\n",
    "                x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n",
    "\n",
    "            skip_outputs, updated_queues = [], []\n",
    "            for i, (conv_input, queue, dilation) in enumerate(zip(conv_inputs, queues, self.dilations)):\n",
    "\n",
    "                state = queue.read(time)\n",
    "                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n",
    "                    w_conv = tf.get_variable('weights'.format(i))\n",
    "                    b_conv = tf.get_variable('biases'.format(i))\n",
    "                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n",
    "                conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n",
    "                dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "                with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n",
    "                    w_proj = tf.get_variable('weights'.format(i))\n",
    "                    b_proj = tf.get_variable('biases'.format(i))\n",
    "                    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n",
    "                skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n",
    "\n",
    "                x_proj += residuals\n",
    "                skip_outputs.append(skips)\n",
    "                updated_queues.append(queue.write(time + dilation, x_proj))\n",
    "\n",
    "            skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=1))\n",
    "            with tf.variable_scope('dense-decode-1', reuse=True):\n",
    "                w_h = tf.get_variable('weights')\n",
    "                b_h = tf.get_variable('biases')\n",
    "                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n",
    "\n",
    "            with tf.variable_scope('dense-decode-2', reuse=True):\n",
    "                w_y = tf.get_variable('weights')\n",
    "                b_y = tf.get_variable('biases')\n",
    "                y_hat = tf.matmul(h, w_y) + b_y\n",
    "\n",
    "            elements_finished = (time >= self.decode_len)\n",
    "            finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "            next_input = tf.cond(\n",
    "                finished,\n",
    "                lambda: tf.zeros([batch_size, 1], dtype=tf.float32),\n",
    "                lambda: y_hat\n",
    "            )\n",
    "            next_elements_finished = (time >= self.decode_len - 1)\n",
    "\n",
    "            return (next_elements_finished, next_input, updated_queues)\n",
    "\n",
    "        def condition(unused_time, elements_finished, *_):\n",
    "            return tf.logical_not(tf.reduce_all(elements_finished))\n",
    "\n",
    "        def body(time, elements_finished, emit_ta, *state_queues):\n",
    "            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n",
    "\n",
    "            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n",
    "            emit_ta = emit_ta.write(time, emit)\n",
    "\n",
    "            elements_finished = tf.logical_or(elements_finished, next_finished)\n",
    "            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n",
    "\n",
    "        returned = tf.while_loop(\n",
    "            cond=condition,\n",
    "            body=body,\n",
    "            loop_vars=[time, elements_finished, emit_ta] + state_queues\n",
    "        )\n",
    "\n",
    "        outputs_ta = returned[2]\n",
    "        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n",
    "        return y_hat\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "\n",
    "        y_hat_encode, conv_inputs = self.encode(x, features=self.encode_features)\n",
    "        self.initialize_decode_params(x, features=self.decode_features)\n",
    "        y_hat_decode = self.decode(y_hat_encode, conv_inputs, features=self.decode_features)\n",
    "        y_hat_decode = self.inverse_transform(tf.squeeze(y_hat_decode, 2))\n",
    "        y_hat_decode = tf.nn.relu(y_hat_decode)\n",
    "\n",
    "        self.labels = self.y_decode\n",
    "        self.preds = y_hat_decode\n",
    "        self.loss = sequence_smape(self.labels, self.preds, self.decode_len, self.is_nan_decode)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'priors': self.x_encode,\n",
    "            'labels': self.labels,\n",
    "            'preds': self.preds,\n",
    "            'page_id': self.page_id,\n",
    "        }\n",
    "\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = DataReader(data_dir=os.path.join(base_dir, 'data/processed/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
